---
title: 'Streaming with FastAPI and Python'
pubDate: 2023-12-08
description: ''
image:
  {
    src: '../../assets/reads/python_fastapi_event_stream.jpg',
    alt: 'A cartoon Python snake creatively intertwined with a flowing stream of text and data, symbolizing text/event-stream. The Python snake is depicted with a friendly and intelligent look, interacting with the stream. In the background, a simplified representation of the FastAPI logo, with its characteristic green and white colors, is visible. The overall atmosphere is tech-savvy and engaging',
  }
tags: ['python', 'gpt', 'openai', 'fastapi']
---

import Image from '../../components/reads/Image.astro'
import coverImage from '../../assets/reads/python_fastapi_event_stream.jpg'

Exactly 7 months ago today I published an article about [streaming on the edge with Deno](/reads/streaming-on-the-edge). That time I worked on a project that involved streaming OpenAI API reponses to the user like ChatGPT does.

For our upcoming project instead of using Supabase as the backend, we are going to use Python and FastAPI. Personally I have never coded anything in Python other than some basic proof of concept stuff, so I am excited to learn something new.

<Image
  src={coverImage}
  alt="A cartoon Python snake creatively intertwined with a flowing stream of text and data, symbolizing text/event-stream. The Python snake is depicted with a friendly and intelligent look, interacting with the stream. In the background, a simplified representation of the FastAPI logo, with its characteristic green and white colors, is visible. The overall atmosphere is tech-savvy and engaging"
  caption="A cartoon Python snake intertwined with a flowing stream of text and data, symbolizing text/event-stream."
  lazy={false}
/>

## Inspiration

When creating our prototype I stumpled upon an article on [tech.clevertap.com](https://tech.clevertap.com/streaming-openai-app-in-python/). Soon I realized that this article was based on the pre 1.0 version of OpenAI's Python library and that the API had changed quite a bit. So I decided to write about what I learned in the process of getting this to work.

## Project setup with JetBrains PyCharm

As you sure know (because - of course - you deeply studied my [Uses](/uses) page), I am a daily Visual Studio Code user. For the upcoming project we as a developent team at my day job decided to use JetBrains PyCharm. And I have to admit that I am quite impressed with the IDE so far.

Setting up a FastAPI project in PyCharm is pretty straigt forward. It provides a project template and you can choose to use a virtual environment. `uvicorn` is the server of choice for FastAPI and it was already configured in the project template. Thumbs up for that! üëç

### Dependencies

Dependencies required for this dummy project are also pretty straight forward. We need `fastapi`, `python-dotenv` and `openai`. The latter is the official OpenAI Python library. And of course we need `uvicorn` as the server.

```bash
# requirements.txt
fastapi
openai
python-dotenv
uvicorn
```

I added the `python-dotenv` in order read my OpenAI API key from a `.env` file by calling `load_dotenv()` in the `main.py` file.

## The OpenAI API client

Since version 1.0.0 the OpenAI Python library has changed quite a bit. First of all you need to instantiate a client in order to use the API.

```python
# main.py

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

if not len(OPENAI_API_KEY):
    print("Please set OPENAI_API_KEY environment variable. Exiting.")
    sys.exit(1)

client = OpenAI()
client.api_key = OPENAI_API_KEY
```

More about the recent Python API changes you can find in the [v1.0.0 Migration Guide](https://github.com/openai/openai-python/discussions/742).

### Streaming completions

In order to stream completions from the OpenAI API you need to set the `stream` parameter to `True` when calling `client.chat.completions.create()`. This will return a generator that you can iterate over and yield the response chunks in our own response.

```python
# main.py

def get_response_openai(prompt):
  try:
    prompt = prompt
    completion = client.chat.completions.create(
      model="gpt-3.5-turbo",
      n=1,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0,
      messages=[
        {"role": "system",
          "content": "You are an expert creative marketer. Create a campaign for the brand the user enters."},
        {"role": "user", "content": prompt},
      ],
      stream=True,
    )
  except Exception as e:
    print("Error in creating campaigns from OpenAI:", str(e))
    return 503
  try:
    for chunk in completion:
      current_content = chunk.choices[0].delta.content
      if current_content is not None:
        yield current_content
      else:
        continue
  except Exception as e:
    print("Error in yielding response chunks from OpenAI:", str(e))
    return 503
```

## FastAPI's `StreamingResponse`

In order to stream back the chunks we receive from the OpenAI API we need to use FastAPI's `StreamingResponse`. This is a class that takes a generator as its first parameter and yields the chunks to the client.

The following is the whole API endpoint that we need to create in order to stream back the OpenAI API responses to the client.

```python
# main.py

@app.get(
  "/campaign/",
  tags=["APIs"],
  response_model=str,
)
def campaign(prompt: str = Query(..., max_length=20)):
  return StreamingResponse(get_response_openai(prompt), media_type="text/event-stream")
```

## Conclusion

I am really impressed with FastAPI and Python so far. It is a great combination for building APIs. It should be quite performant and the code is super readable. Projects built with FastAPI seem to be quite maintainable, too. This is why we opted for using something lightweight like FastAPI instead of a full blown Django project.