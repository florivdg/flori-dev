---
title: "Offline RAG with Bun, libSQL, LangChain.js, and Ollama"
pubDate: 2024-10-25
description: "How might a simple, personal and offline-only RAG chatbot look like? A command-line tool that would import a single PDF at a time and allow me to chat with its contents was the goal. Being comfortable writing TypeScript, I decided to give it a go using Bun, libSQL, LangChain.js and Ollama."
image:
  {
    src: '../../assets/reads/chatbot-pulls-plug-offline-rag-with-ollama.jpg',
    alt: 'A cute, glowing robot sitting in front of a large cloud icon with an ‚ÄúX‚Äù symbol inside it. The robot has a simple design, featuring a round head, antennae, and a friendly smile. It is connected to various devices through neon-lit wires. The overall color scheme is a mix of neon blues and purples, creating a futuristic, digital environment with cloud computing elements.',
  }
tags: ["RAG", "Bun", "libSQL", "LangChain", "Ollama"]
---

import Image from '../../components/reads/Image.astro'
import Note from '../../components/reads/Note.astro'
import coverImage from '../../assets/reads/chatbot-pulls-plug-offline-rag-with-ollama.jpg'

When Meta released its Llama 3.2 model, I wondered what a simple, personal and offline-only RAG (retrieval augmented generation) chatbot might look like. A command-line tool that would import a single PDF at a time and allow me to chat with its contents was the goal. Being comfortable writing TypeScript, I decided to give it a go using Bun, libSQL, LangChain.js and Ollama. Here's how it went.

<Image
  src={coverImage}
  alt='A cute, glowing robot sitting in front of a large cloud icon with an ‚ÄúX‚Äù symbol inside it. The robot has a simple design, featuring a round head, antennae, and a friendly smile. It is connected to various devices through neon-lit wires. The overall color scheme is a mix of neon blues and purples, creating a futuristic, digital environment with cloud computing elements.'
  caption="Pulling the Plug on Cloud Dependency: How libSQL, LangChain, and Ollama Empower Offline Retrieval-Augmented Generation."
  lazy={false}
/>

## The Building Blocks

* **Bun**: The new JS kid on the block. TypeScript-first, *blazingly fast* üòú, and has a `package.json`. Definitely my go-to JS runtime for side projects.
* **libSQL**: A fork of SQLite that has a vector type and vector queries built-in. Battle-tested on [Turso](https://turso.tech) but also a perfect local-first, file-based database for AI powered apps.
* **LangChain.js**: The JavaScript port of the popular LangChain python library. Brings everything seamlessly together: Document loading, chunking, embeddings, vector stores, and retrieval.
* **Ollama**: The best way to run open source AI models locally on your computer. You're up and running with one simple command.

## But then, LangChain Problems

After I started implementing loading a PDF, chunking it and creating the embeddings for the chunks, I immediately ran into problems with LangChain. Namely the `@langchain/community` package from which I imported the `LibSQLVectorStore`.

It threw an error when trying to insert documents into the database. At first I thought there was something wrong with my chunking and embedding functions, but soon I realized there was an actual bug in the `@langchain/community` package. I filed an [issue on GitHub](https://github.com/langchain-ai/langchainjs/issues/7040) and a [PR with a proposed fix](https://github.com/langchain-ai/langchainjs/pull/7041).

Turns out the `LibSQLVectorStore` implementation wasn't quite finished, but somehow made it into a release. Mainter [@jacoblee93](https://github.com/jacoblee93) helped fix a few more issues and also wrote some integration tests.

Now back to work... ü§ì

## Creating Embeddings Locally

Back in April Ollama [announced support for embeddings](https://ollama.com/blog/embedding-models). That's great because now with Ollama we are able to run the complete retrieval chain including embedding documents and user queries as well as the LLM for chatting locally on our own computer. No need to rely on OpenAI and the like anymore.

### Choosing the Embeddings Model

You can find every supported embeddings model in the [Ollama library](https://ollama.com/search?c=embedding). Since I wanted to get started quickly I simply chose `nomic-embed-text` since it had the most downloads and looked like a good fit for my use case.

<Note>**üî• Hot Tip:** Check out Hugging Face's [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for comparing open source embeddings models and finding the right one for your needs.</Note>

## How to Store Vectors Locally?

Generating embeddings is quite a time consuming task. So I wanted to store them locally to avoid having to recompute them every time I start the chatbot. But you also need to be able to query them efficiently.

By now there are a million proprierty as well as open source databases that support vectors. But I wanted to keep it simple and local. That's why I chose [libSQL](https://github.com/tursodatabase/libsql). It's a fork of SQLite that has a vector type, vector indices and vector queries built-in. Perfect for my offline RAG chatbot.

<Note>**üî• Hot Tip:** Have a look into [`pgvector`](https://github.com/pgvector/pgvector), a great extension that brings full support for vectors to Postgres. Also checkout [Supabase](https://supabase.com/) for a cloud solution based on Postgres and `pgvector`.</Note>

### Help, my Database is Huge!

While testing, I used the Brexit Withdrawal Agreement as it is quite long and therefore looked like a good choice to do some retrieval tests. It took me quite a while to realize that with this one document alone, the libSQL file was already more than 500MB in size. ü§Ø

The PDF of the Brexit agreement is 181 pages, but that's still too much. If I were to use this chatbot with multiple documents, the database file would quickly grow to several gigabytes. That's not really what I had in mind for a simple, personal, offline-only chatbot.

Fortunately, I found this article about [*The space complexity of vector indexes in LibSQL*](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql) on the Turso blog.

#### Compression and Limiting To The Rescue

The main problem was not the vectors themselves, but the vector index. By limiting the number of neighbors and compressing them, the database file size was significantly reduced. Here's the updated SQL command:

```sql {5}
# db.sql 
CREATE INDEX IF NOT EXISTS
  idx_vecs_embeddings ON vecs(
    libsql_vector_idx(
      embeddings, 'compress_neighbors=float8', 'max_neighbors=20'
    )
  );
```

As the Turso article explains, setting the compression to `float8` and the maximum number of neighbors to `20` would still give good results. I can also confirm that I haven't noticed any difference in the quality of the retrieval results so far. But the file size dropped from over 500 MB to under 80 MB. üéâ