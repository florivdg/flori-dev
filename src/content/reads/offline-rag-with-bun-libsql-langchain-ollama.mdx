---
title: "Offline RAG with Bun, libSQL, LangChain.js, and Ollama"
pubDate: 2024-10-25
description: "How might a simple, personal and offline-only RAG chatbot look like? A command-line tool that would import a single PDF at a time and allow me to chat with its contents was the goal. Being comfortable writing TypeScript, I decided to give it a go using Bun, libSQL, LangChain.js and Ollama."
image:
  {
    src: '../../assets/reads/chatbot-pulls-plug-offline-rag-with-ollama.jpg',
    alt: 'A cute, glowing robot sitting in front of a large cloud icon with an ‚ÄúX‚Äù symbol inside it. The robot has a simple design, featuring a round head, antennae, and a friendly smile. It is connected to various devices through neon-lit wires. The overall color scheme is a mix of neon blues and purples, creating a futuristic, digital environment with cloud computing elements.',
  }
tags: ["RAG", "Bun", "libSQL", "LangChain", "Ollama"]
---

import Image from '../../components/reads/Image.astro'
import Note from '../../components/reads/Note.astro'
import coverImage from '../../assets/reads/chatbot-pulls-plug-offline-rag-with-ollama.jpg'

When Meta released its Llama 3.2 model, I wondered what a simple, personal and offline-only RAG (retrieval augmented generation) chatbot might look like. A command-line tool that would import a single PDF at a time and allow me to chat with its contents was the goal. Being comfortable writing TypeScript, I decided to give it a go using Bun, libSQL, LangChain.js and Ollama. Here's how it went.

<Image
  src={coverImage}
  alt='A cute, glowing robot sitting in front of a large cloud icon with an ‚ÄúX‚Äù symbol inside it. The robot has a simple design, featuring a round head, antennae, and a friendly smile. It is connected to various devices through neon-lit wires. The overall color scheme is a mix of neon blues and purples, creating a futuristic, digital environment with cloud computing elements.'
  caption="Pulling the Plug on Cloud Dependency: How libSQL, LangChain, and Ollama Empower Offline Retrieval-Augmented Generation."
  lazy={false}
/>

## The Building Blocks

* **Bun**: The new JS kid on the block. TypeScript-first, *blazingly fast* üòú, and has a `package.json`. Definitely my go-to JS runtime for side projects.
* **libSQL**: A fork of SQLite that has a vector type and vector queries built-in. Battle-tested on [Turso](https://turso.tech) but also a perfect local-first, file-based database for AI powered apps.
* **LangChain.js**: The JavaScript port of the popular LangChain python library. Brings everything seamlessly together: Document loading, chunking, embeddings, vector stores, and retrieval.
* **Ollama**: The best way to run open source AI models locally on your computer. You're up and running with one simple command.

## But then, LangChain Problems

After I started implementing loading a PDF, chunking it and creating the embeddings for the chunks, I immediately ran into problems with LangChain. Namely the `@langchain/community` package from which I imported the `LibSQLVectorStore`.

It threw an error when trying to insert documents into the database. At first I thought there was something wrong with my chunking and embedding functions, but soon I realized there was an actual bug in the `@langchain/community` package. I filed an [issue on GitHub](https://github.com/langchain-ai/langchainjs/issues/7040) and a [PR with a proposed fix](https://github.com/langchain-ai/langchainjs/pull/7041).

Turns out the `LibSQLVectorStore` implementation wasn't quite finished, but somehow made it into a release. Mainter [@jacoblee93](https://github.com/jacoblee93) helped fix a few more issues and also wrote some integration tests.

Now back to work... ü§ì

## Creating Embeddings Locally

Back in April Ollama [announced support for embeddings](https://ollama.com/blog/embedding-models). That's great because now with Ollama we are able to run the complete retrieval chain including embedding documents and user queries as well as the LLM for chatting locally on our own computer. No need to rely on OpenAI and the like anymore.

### Choosing the Embeddings Model

You can find every supported embeddings model in the [Ollama library](https://ollama.com/search?c=embedding). Since I wanted to get started quickly I simply chose `nomic-embed-text` since it had the most downloads and looked like a good fit for my use case.

<Note>**üî• Hot Tip:** Check out Hugging Face's [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for comparing open source embeddings models and finding the right one for your needs.</Note>

## How to Store Vectors Locally?

Generating embeddings is quite a time consuming task. So I wanted to store them locally to avoid having to recompute them every time I start the chatbot. But you also need to be able to query them efficiently.

By now there are a million proprierty as well as open source databases that support vectors. But I wanted to keep it simple and local. That's why I chose [libSQL](https://github.com/tursodatabase/libsql). It's a fork of SQLite that has a vector type, vector indices and vector queries built-in. Perfect for my offline RAG chatbot.

<Note>**üî• Hot Tip:** Have a look into [`pgvector`](https://github.com/pgvector/pgvector), a great extension that brings full support for vectors to Postgres. Also checkout [Supabase](https://supabase.com/) for a cloud solution based on Postgres and `pgvector`.</Note>

### Help, my Database is Huge!

While testing, I used the Brexit Withdrawal Agreement as it is quite long and therefore looked like a good choice to do some retrieval tests. It took me quite a while to realize that with this one document alone, the libSQL file was already more than 500 MB in size. ü§Ø

The PDF of the Brexit agreement is 181 pages, but that's still too much. If I were to use this chatbot with multiple documents, the database file would quickly grow to several gigabytes. That's not really what I had in mind for a simple, personal, offline-only chatbot.

Fortunately, I found this article about [*The space complexity of vector indexes in LibSQL*](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql) on the Turso blog.

#### Compression and Limiting To The Rescue

The main problem was not the vectors themselves, but the vector index. By limiting the number of neighbors and compressing them, the database file size was significantly reduced. Here's the updated SQL command:

```sql {5}
# db.sql 
CREATE INDEX IF NOT EXISTS
  idx_vecs_embeddings ON vecs(
    libsql_vector_idx(
      embeddings, 'compress_neighbors=float8', 'max_neighbors=20'
    )
  );
```

As the Turso article explains, setting the compression to `float8` and the maximum number of neighbors to `20` would still give good results. I can also confirm that I haven't noticed any difference in the quality of the retrieval results so far. But the file size dropped from over 500 MB to under 80 MB. üéâ

## Bringing It All Together

With the libSQL implementation in LangChain fixed, the embeddings model chosen and the database file size reduced, I was finally able to chat with the Brexit Withdrawal Agreement. üéâ

Now, how does it all come together? Enter: LangChain.js.

### Loading and Chunking Documents

LangChain.js takes care of loading the document. All you need is `pdf-parse` installed as a peer dependency and a path to a PDF file:

```typescript
# loader.ts
import { PDFLoader } from '@langchain/community/document_loaders/fs/pdf'

export async function load(path: string) {
  const loader = new PDFLoader(path)
  return await loader.load()
}
```

By default this returns one `Document` object for each page of the PDF file. Since the context windows of LLMs grow constantly, this may be good enough for your use case.

<Note>**‚òùÔ∏è Good To Know:** The Llama 3.2 1B and 3B models support a context length of **128K** tokens.</Note>

Chunking can still greatly improve retrieval results. For example, when creating embeddings for smaller parts of the document, the vectors are more likely to represent the embedded content more accurately than for an entire page.

For simplicity I decided to split the documents with a `RecursiveCharacterTextSplitter`. It basically splits the text into chunks of a given number of tokens each, but tries to keep sentences together.

```typescript {7-9}
# splitter.ts
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'
import type { Document } from 'langchain/document'

const CHUNK_SIZE = 250

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: CHUNK_SIZE,
})

export async function split(docs: Document[]) {
  return await splitter.splitDocuments(docs)
}
```

<Note>**üî• Hot Tip:** Try semantic chunking to make retrieval even more accurate by keeping related content together.</Note>

### The Vector Store

The `LibSQLVectorStore` is the heart of the retrieval process. It stores the embeddings of the document chunks and allows for efficient retrieval of the most similar chunks to a given query, which than can be used to let the Llama model generate a response.

In order to create the vector store, you need to provide a database client as well as an embeddings model. The vector store will then be responsible for storing the embeddings in the database and also retrieving them later.

```typescript {5}
# vectorstore.ts
const embeddings = new OllamaEmbeddings({ model: 'nomic-embed-text' })

const libsqlClient = createClient({
  url: 'file:vector-store.db',
})

export const vectorStore = new LibSQLVectorStore(embeddings, {
  db: libsqlClient,
  table: 'vecs',
  column: 'embeddings',
})
```

In the highlighted line above you can see that we provide `file:vector-store.db` as the URL for the database. This tells libSQL to create a local database file in the current directory.

#### Creating Tables and Indices

Before you can start storing vectors in the database, you need to create tables and indices.

Make sure that your database schema matches the one you provided to the `LibSQLVectorStore` constructor as `table` and `column`.

```sql {6}
# db.sql
CREATE TABLE IF NOT EXISTS vecs (
    id INTEGER PRIMARY KEY AUTOINCREMENT, 
    content TEXT, 
    metadata TEXT, 
    embeddings F32_BLOB(768)
);
```

<Note>The `F32_BLOB(768)` type matches the vector size of the `nomic-embed-text` model. If you choose a different model, be sure to adjust the size accordingly.</Note>

#### Keeping Track of Files

We also have a `files` table that stores the paths of the documents. This is useful to check if a document has already been processed and to avoid re-embedding it. The file ID is stored in the `metadata` column of the `vecs` table for seamless filtering with the built-in retriever of the `LibSQLVectorStore`.

```typescript {11}
# vectorstore.ts
// Insert the file into the database and get the ID
const { rows } = await libsqlClient.execute({
  sql: `INSERT INTO files (filename, path)
        VALUES (?, ?) RETURNING id;`,
  args: [fileName, filePath],
})

// Add the file ID to the metadata of each document chunk
const docsWithMetadata = docs.map((doc) =>
  ({ ...doc, metadata: { file_id: rows[0].id } }))

// Store the documents with metadata in the vector store
await vectorStore.addDocuments(docsWithMetadata)
```